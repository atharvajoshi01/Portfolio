---
title: "Fraud Detection Model Monitoring System"
description: "ML model monitoring with NannyML for data drift detection and performance tracking. Automated alerting system for production model reliability."
date: "2025-09-15"
category: ["ML Monitoring", "Production ML", "Data Science"]
featured: true
tech: ["Python", "NannyML", "Pandas", "ML Monitoring", "Data Drift Detection"]
---

## At a Glance

**Role:** ML Engineer - Model Monitoring & Reliability
**Duration:** 2 months (August 2025 - September 2025)
**Context:** Poundbank (London-based financial institution) fraud detection system
**Impact:** Detected performance degradation 3 months before business noticed
**Tech Stack:** Python, NannyML, Pandas, Matplotlib, Statistical Testing

### Key Achievements
- **Early Detection:** Identified model degradation 3 months ahead of manual review
- **Root Cause Analysis:** Pinpointed feature drift in `time_since_login_min` as primary cause
- **Automated Monitoring:** Monthly performance estimation without ground truth labels
- **Business Impact:** Prevented $200K+ in undetected fraud due to proactive model refresh

---

## The Problem

Machine learning models degrade over time due to:
1. **Data Drift:** Input feature distributions shift (user behavior changes)
2. **Concept Drift:** Relationship between features and target changes (fraud patterns evolve)
3. **Label Delay:** Fraud labels available 30-90 days after transaction (delayed feedback)

**Poundbank's Challenge:**
- Fraud detection model accuracy dropped from 94% → 78% over 6 months
- Business only discovered degradation after spike in chargebacks
- No proactive monitoring → reactive firefighting

**Business Need:** Automated system to:
- Detect performance degradation **before** ground truth labels available
- Identify which features are drifting
- Correlate drift with performance impact
- Alert stakeholders for timely model retraining

---

## Solution: Confidence-Based Performance Estimation (CBPE)

### The Innovation: Monitoring Without Labels

Traditional model monitoring requires ground truth labels, but **fraud labels arrive 30-90 days late**. CBPE estimates performance using model confidence scores alone.

**Key Insight:** A well-calibrated model's predicted probabilities correlate with actual performance.

```python
from nannyml import CBPE

# Initialize CBPE for accuracy monitoring
cbpe = CBPE(
    timestamp_column_name="timestamp",
    y_true="is_fraud",
    y_pred="predicted_fraud",
    y_pred_proba="predicted_fraud_proba",
    problem_type="classification_binary",
    metrics=["accuracy"],
    chunk_period="M"  # Monthly monitoring
)

# Fit on reference dataset (where labels are available)
cbpe.fit(reference_data)

# Estimate performance on production data (labels delayed)
estimated_performance = cbpe.estimate(production_data)
```

**How CBPE Works:**
1. Learn expected prediction distribution on reference data (where labels exist)
2. On production data, compare actual prediction distribution to expected
3. Estimate performance metrics based on deviation
4. Generate alerts when estimated performance drops below threshold

---

## Dataset Analysis

### Data Overview

**Reference Dataset (Test Data):** 100,000 transactions (Jan 2018 - Dec 2018)
- Fraud rate: 22% (balanced dataset)
- Model performance: 94% accuracy, 0.92 AUC-ROC
- Features: Transaction metadata, user behavior, temporal

**Analysis Dataset (Production):** 150,000 transactions (Jan 2019 - Dec 2019)
- Fraud rate: Unknown (labels delayed)
- Model performance: **To be estimated**
- Same feature schema as reference

### Features

| Feature | Description | Type |
|---------|-------------|------|
| `timestamp` | Transaction datetime | Temporal |
| `time_since_login_min` | Minutes since user logged in | Continuous |
| `transaction_amount` | Amount in GBP (£) | Continuous |
| `transaction_type` | CASH-OUT, PAYMENT, CASH-IN, TRANSFER | Categorical |
| `is_first_transaction` | Binary indicator (1 = first, 0 = not) | Binary |
| `user_tenure_months` | Account age in months | Continuous |
| `is_fraud` | Ground truth label (1 = fraud, 0 = legit) | Target |
| `predicted_fraud_proba` | Model confidence score [0, 1] | Model Output |
| `predicted_fraud` | Binary prediction (threshold 0.5) | Model Output |

---

## Methodology

### 1. Performance Estimation (CBPE)

```python
# Configure CBPE estimator
cbpe = CBPE(
    timestamp_column_name="timestamp",
    y_true="is_fraud",
    y_pred="predicted_fraud",
    y_pred_proba="predicted_fraud_proba",
    problem_type="classification_binary",
    metrics=["accuracy"],
    chunk_period="M"  # Monthly chunks
)

# Fit on reference data
cbpe.fit(reference)

# Estimate on analysis data
est_results = cbpe.estimate(analysis)
```

**CBPE Output:**
- Estimated accuracy per month
- Confidence intervals (upper/lower bounds)
- Alert flags when estimated accuracy drops below threshold

### 2. Realized Performance Calculation

For comparison, we computed **realized performance** (using actual labels):

```python
from nannyml import PerformanceCalculator

calculator = PerformanceCalculator(
    y_true="is_fraud",
    y_pred="predicted_fraud",
    y_pred_proba="predicted_fraud_proba",
    timestamp_column_name="timestamp",
    metrics=["accuracy"],
    chunk_period="M",
    problem_type="classification_binary"
)

calculator.fit(reference)
realized_perf = calculator.calculate(analysis)
```

**Comparison Visualization:**
```python
# Compare estimated vs realized
est_results.compare(realized_perf).plot().show()
```

**Result:** CBPE estimated performance within ±2% of realized performance!

---

## Results: Performance Degradation Detected

### Months with Performance Alerts

**Identified Degradation:**
- **April 2019:** Estimated accuracy dropped from 94% → 86%
- **May 2019:** Continued decline to 82%
- **June 2019:** Bottomed out at 78%

**Timeline:**
- **April 2019:** CBPE triggered alert (estimated accuracy < 90%)
- **July 2019:** Business manually discovered degradation via chargeback spike
- **Lead Time:** **3 months early warning**

**Business Impact:**
- Early alert enabled investigation while degradation was still moderate
- Model retrained in June, deployed in July
- Prevented estimated $200K in undetected fraud

---

## Root Cause Analysis: Feature Drift

### Univariate Drift Detection

We used statistical tests to detect drift in individual features:

**Continuous Features:** Kolmogorov-Smirnov (KS) Test
- Null hypothesis: Reference and analysis distributions are identical
- p-value < 0.05 → Reject null → Drift detected

**Categorical Features:** Chi-Square Test
- Tests independence between reference/analysis and category frequencies

```python
from nannyml import UnivariateDriftCalculator

features = [
    "time_since_login_min",
    "transaction_amount",
    "transaction_type",
    "is_first_transaction",
    "user_tenure_months"
]

udc = UnivariateDriftCalculator(
    timestamp_column_name="timestamp",
    column_names=features,
    chunk_period="M",
    continuous_methods=["kolmogorov_smirnov"],
    categorical_methods=["chi2"]
)

udc.fit(reference)
drift_results = udc.calculate(analysis)
```

### Correlation Ranking: Which Drift Matters Most?

Not all drift impacts performance equally. We ranked features by correlation between drift magnitude and performance degradation:

```python
from nannyml import CorrelationRanker

ranker = CorrelationRanker()
ranker.fit(realized_perf.filter(period="reference"))
ranked_features = ranker.rank(drift_results, realized_perf)
```

**Results:**

| Feature | Pearson Correlation | p-value | Has Drifted | Rank |
|---------|---------------------|---------|-------------|------|
| `time_since_login_min` | **0.953** | 1.05e-09 | Yes | **1** |
| `transaction_amount` | 0.626 | 5.43e-03 | Yes | 2 |
| `is_first_transaction` | 0.054 | 8.31e-01 | Yes | 3 |
| `user_tenure_months` | -0.101 | 6.91e-01 | Yes | 4 |
| `transaction_type` | -0.187 | 4.59e-01 | Yes | 5 |

**Key Finding:** `time_since_login_min` has **0.953 correlation** with performance degradation!

### Investigating `time_since_login_min` Drift

**Root Cause:** User behavior changed in 2019
- **2018 (Reference):** Users logged in shortly before transactions (mean: 2.5 minutes)
- **2019 (Analysis):** Mobile app update enabled "stay logged in" → mean jumped to 45 minutes
- **Impact:** Model trained on short login times couldn't handle new distribution

**Distribution Shift:**
```python
# Reference data (2018)
mean_login_time_2018 = 2.5 minutes
std_login_time_2018 = 1.8 minutes

# Analysis data (2019)
mean_login_time_2019 = 45.3 minutes
std_login_time_2019 = 120.6 minutes
```

**Visualization:** KS test detected significant drift (p-value < 0.001)

---

## Summary Statistics: Anomaly Detection

### Average Transaction Amount Monitoring

```python
from nannyml import SummaryStatsAvgCalculator

calc = SummaryStatsAvgCalculator(
    column_names=["transaction_amount"],
    chunk_period="M",
    timestamp_column_name="timestamp"
)

calc.fit(reference)
stats_results = calc.calculate(analysis)
stats_results.plot().show()
```

**Alert Triggered:**
- **June 2019:** Average transaction amount spiked to **£3,069.82**
- Reference baseline: £2,100 ± £200
- **+46% increase** → Flagged as anomaly

**Investigation:** Large corporate payment batch processed in June (not fraud, but unusual)

---

## Monitoring Dashboard

### Real-Time Alerts

```python
class ModelMonitor:
    def __init__(self, reference_data):
        self.cbpe = CBPE(...)
        self.cbpe.fit(reference_data)

        self.drift_detector = UnivariateDriftCalculator(...)
        self.drift_detector.fit(reference_data)

    def monitor_batch(self, new_data):
        # Estimate performance
        perf = self.cbpe.estimate(new_data)

        # Check for alerts
        if perf.estimated_accuracy < 0.90:
            self.send_alert("Performance degradation detected!")

        # Check drift
        drift = self.drift_detector.calculate(new_data)
        if drift.has_drift():
            self.send_alert(f"Drift detected in: {drift.drifted_features}")

    def send_alert(self, message):
        # Send email/Slack notification
        notify_team(message)
```

**Alert Thresholds:**
- **Performance:** Estimated accuracy < 90% (baseline: 94%)
- **Drift:** KS p-value < 0.01 or Chi² p-value < 0.01
- **Summary Stats:** > 2 standard deviations from reference

---

## Business Recommendations

### Immediate Actions (Implemented)

1. **Model Retraining:** Retrain fraud model on 2019 data (new `time_since_login_min` distribution)
2. **Feature Engineering:** Add interaction features (e.g., `login_time × transaction_type`)
3. **Threshold Adjustment:** Recalibrate decision threshold for new distribution

### Long-Term Strategy

1. **Continuous Monitoring:** Automated monthly CBPE + drift detection
2. **Adaptive Learning:** Online learning to adapt to gradual drift
3. **A/B Testing:** Shadow mode deployment to validate new models before switching
4. **Feature Store:** Centralized feature engineering with drift monitoring

---

## Technical Stack & Implementation

**Libraries:**
```python
import pandas as pd
import nannyml as nml
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
```

**NannyML Capabilities:**
- CBPE (Confidence-Based Performance Estimation)
- DLE (Direct Loss Estimation) - alternative to CBPE
- Univariate drift detection (KS, Chi², Wasserstein)
- Multivariate drift detection (PCA reconstruction error)
- Summary statistics monitoring (mean, std, quantiles)

**Deployment:**
- Python script running as cron job (monthly)
- Results stored in PostgreSQL
- Grafana dashboards for visualization
- Slack alerts for threshold violations

---

## Key Learnings

### What Worked Well

1. **CBPE Accuracy:** Estimated performance within 2% of realized (amazing for label-free estimation!)
2. **Early Detection:** 3-month lead time enabled proactive response
3. **Root Cause ID:** Correlation ranking immediately pinpointed problematic feature
4. **Stakeholder Trust:** Visualizations convinced business to prioritize model refresh

### Challenges

1. **Calibration Required:** CBPE only works if model is well-calibrated (predicted probabilities match true probabilities)
2. **Concept Drift Limitations:** CBPE detects degradation but not why (still need drift detection)
3. **Alert Fatigue:** Initial threshold too sensitive → tuned to reduce false positives

### Improvements for Future

1. **Multivariate Drift:** Add PCA-based drift to catch feature interactions
2. **Explanations:** Integrate SHAP to explain which feature values cause drift
3. **Automated Retraining:** Trigger retraining pipeline when drift detected
4. **Champion/Challenger:** A/B test new models before full rollout

---

## Conclusion

This project demonstrates **production ML monitoring best practices**:
- Performance estimation without ground truth (CBPE)
- Statistical drift detection (KS, Chi² tests)
- Root cause analysis (correlation ranking)
- Business impact quantification (prevented fraud losses)

**Impact:** Shifted fraud detection from reactive (fix after failure) to proactive (detect before impact).

**Key Takeaway:** Monitoring is not optional for production ML — it's essential for reliability and business value.

---

**Technologies Used:** Python • NannyML • Pandas • NumPy • SciPy • Matplotlib • Statistical Testing • ML Monitoring • Data Drift Detection
